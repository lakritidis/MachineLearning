{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Datasets\n",
    "\n",
    "The datasets that we studied so far were in the form of samples with numeric features, or of samples with features that can be converted to numbers (e.g. dates, categorical features, one-hot-encoded features, etc.). Thus, the representation of the samples as feature vectors was a straightforward prodecure.\n",
    "\n",
    "In text datasets, the samples do not consist of numerical features, but by textual attributes. Examples of such datasets include Web page collections, e-mail messages, corpora containing user posts, comments, reviews, and so on. These document collections are widely used in both supervised and unsupervised learning applications. For instance, sentiment analysis is the process of detecting positive or negative sentiment in text. It is often used by businesses to detect sentiment in social data, gauge brand reputation, and understand customers. In these fields of research, the so-called Natural Language Processing (NLP) algorithms are of great value.\n",
    "\n",
    "In this notebook we study the basic properties of the text datasets. More specifically, we examine methods for:\n",
    "\n",
    "* clearing and preparing the text data,\n",
    "* creating feature vectors from text documents, and\n",
    "* processing the generated feature vectors (e.g. normalization, standardization etc.). \n",
    "\n",
    "\n",
    "## Large Movie Review Dataset (IMDB)\n",
    "\n",
    "This dataset is primarily used for the binary classification of sentiments in user reviews. It consists of 50000 movie reviews; each review is assigned a binary class label (namely, 0 or 1) that represents the sentiment (negative or positive) of the user about a movie. Half of the reviews are used as a training set for training binary classifiers, whereas the rest 50\\% are used for testing the trained models.\n",
    "\n",
    "The IMDB dataset is publicly accessible at [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "After downloading the dataset, we unzip the compressed dataset to the same directory as this notebook. A folder named `aclImdb` will be created to store the 50000 review files.\n",
    "\n",
    "### Preprocessing step\n",
    "\n",
    "The 50000 review files are not in a suitable format to be imported to a machine learning algorithm. The most significant problem is their population; opening, reading and closing such a large number of files will lead to severe performance degradation.\n",
    "\n",
    "For this reason, packing these 50000 reviews into a single CSV file would be a far more preferable alternative. In this CSV file each line will include the text of the review, accompanied by a binary value (0 or 1) that will indicate whether the review is negative or positive. Eventually, the CSV file will contain 50000 lines of `(Review Text, Class Label)` pairs. \n",
    "\n",
    "Before we start, we import a special Python library, 'pyprind' that allows us to use a simple progress bar in the text mode. The library can be installed via the Anaconda Powershell prompt, by typing the command `conda install -c conda-forge pyprind`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:09\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I saw this film on September 1st, 2005 in Indi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Maybe I'm reading into this too much, but I wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I felt this film did have many good qualities....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This movie is amazing because the fact that th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Quitting\" may be as much about exiting a pre-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1\n",
       "3  I saw this film in a sneak preview, and it is ...          1\n",
       "4  Bill Paxton has taken the true story of the 19...          1\n",
       "5  I saw this film on September 1st, 2005 in Indi...          1\n",
       "6  Maybe I'm reading into this too much, but I wo...          1\n",
       "7  I felt this film did have many good qualities....          1\n",
       "8  This movie is amazing because the fact that th...          1\n",
       "9  \"Quitting\" may be as much about exiting a pre-...          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Use 2 decimal points\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# This is the folder that sortes the 50k files of the dataset. It must reside in the same location as this notebook.\n",
    "basepath = \"datasets\\\\aclImdb\"\n",
    "\n",
    "# We open the 50k files and we load their content into a dataframe with 2 columns:\n",
    "# the review text and the binary class of the sentiment.\n",
    "labels = { \"pos\" : 1, \"neg\" : 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in (\"test\", \"train\"):\n",
    "    for l in (\"pos\", \"neg\"):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), \"r\", encoding = \"utf-8\") as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = [\"review\", \"sentiment\"]\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31902</th>\n",
       "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30346</th>\n",
       "      <td>Nathan Detroit (Frank Sinatra) is the manager ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12363</th>\n",
       "      <td>To understand \"Crash Course\" in the right cont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32490</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26128</th>\n",
       "      <td>This movie is directed by Renny Harlin the fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "19602  OK... so... I really like Kris Kristofferson a...          0\n",
       "45519  ***SPOILER*** Do not read this, if you think a...          0\n",
       "25747  hi for all the people who have seen this wonde...          1\n",
       "42642  I recently bought the DVD, forgetting just how...          0\n",
       "31902  Leave it to Braik to put on a good show. Final...          1\n",
       "30346  Nathan Detroit (Frank Sinatra) is the manager ...          1\n",
       "12363  To understand \"Crash Course\" in the right cont...          1\n",
       "32490  I've been impressed with Chavez's stance again...          1\n",
       "26128  This movie is directed by Renny Harlin the fin...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now store the contents of the dataset into a CSV file. In this way, we will replace the 50000 review files by a single CSV file. Moreover, we save the effort of repeating this process over and over again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"datasets\\\\aclImdb.csv\", index=False, encoding = \"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative starting point\n",
    "\n",
    "Since the CSV file has been created, the next time that we open this notebook we can start its execution from the current point. We can use the [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) method of pandas to read its contents directly into a pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nathan Detroit (Frank Sinatra) is the manager ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>To understand \"Crash Course\" in the right cont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This movie is directed by Renny Harlin the fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0\n",
       "5  Leave it to Braik to put on a good show. Final...          1\n",
       "6  Nathan Detroit (Frank Sinatra) is the manager ...          1\n",
       "7  To understand \"Crash Course\" in the right cont...          1\n",
       "8  I've been impressed with Chavez's stance again...          1\n",
       "9  This movie is directed by Renny Harlin the fin...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"datasets\\\\aclImdb.csv\", encoding = \"utf-8\")\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [shape](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.shape.html) method returns the dataframe dimensions formatted as a pair of the form (number of rows, number of columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bag-of-words model\n",
    "\n",
    "Notice that the dataset is still not in the desired format to be used by a machine learning algorithm. In particular, the input samples (that is, the documents) do not consist of features with numeric values. They only include their words and there is no indication on what their \"weight\" is. Therefore, it is impossible to construct arithmetic vectors directly from this form.\n",
    "\n",
    "The _bag-of-words (BOW)_ model allows the representation of text data as numeric attribute vectors. The idea behind BOW is quite simple and can be summarized as follows:\n",
    "\n",
    "1. We create a dictionary that will contain all the words in the document collection. Each word will be accompanied by some statistics such as its frequency in the dataset, the number of documents that include it, etc.\n",
    "2. The vector representation of each document will include as many components as the words contained in the document. Each component is quantified by utilizing the aforementioned statistics, or combinations of them as we will see below.\n",
    "\n",
    "Since the unique words in each document represent only a small subset of all the words in the dictionary, the generated feature vectors will consist mainly of zeros. These types of vectors are called _sparse vectors_.\n",
    "\n",
    "## Simple document vectorization with `CountVectorizer()`\n",
    "\n",
    "Let us demonstrate a simple document vectorization technique by constructing a toy dataset made up of the 3 following documents:\n",
    "\n",
    "1. The sun is shining\n",
    "2. The weather is sweet\n",
    "3. The sun is shining, the weather is sweet, and one and one is two\n",
    "\n",
    "We create a standard NumPy array to accommodate this small dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy dataset of 3 documents\n",
    "toy_dataset = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) object of scikit-learn transforms a text dataset into a Python dictionary that contains all the unique words in the document collection, accompanied by an integer value.\n",
    "\n",
    "More specifically, the `fit_transform()` method of [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) initially creates a data structure (which we call the *bag-of-words*) and then, it uses this structure to transform the three sentences into sparse vectors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Create the bag-of-words with CountVectorizer()\n",
    "bag = count_vectorizer.fit_transform(toy_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we display the contents of the dictionary by accessing the `vocabulary_` member of `count_vectorizer`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "# Display the dictionary that is used for vectorization\n",
    "print(count_vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary contains all the distinct words in the collection. Each word is accompanied by an integer value that will be used by `CountVectorizer()` to construct the feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Print the feature vectors\n",
    "print(bag.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us explain the abovementioned results. The first element to discuss is that all the three feature vectors have the same dimensionality. Moreover, the number of their components (9) is equal to the number of words in the dictionary (also 9). So, regardless of the number of words a document actually contains, the resulting feature vector will include as many components, as the unique words in the dictionary.\n",
    "\n",
    "The second issue is related to the indices of the vectors. In short, the $i$-th element in the vector represents the word with value $i$ in the dictionary. For example, the element with index 0 in the vector (namely, the first element) represents the word with value 0 in the dictionary: this is the word \"*and*\". Similarly, the element with index 5 in the vector represents the word with value 5 in the dictionary and this is the word \"*sweet*\".\n",
    "\n",
    "The third issue has to do with the value of each element in the vector: This value represents the number of the occurrences of the corresponding word in the respective document. For example, let us examine the second element of the third vector: its index is 1 and according to the dictionary, it corresponds to the word \"*is*\". Now the value 3 tells us that the word \"*is*\" appears three times in document 3.\n",
    "\n",
    "Notice that in the relevant literature, the aforementioned word counts are also called as *term frequencies* $\\text{tf} (t, d)$.\n",
    "\n",
    "\n",
    "## Improved document vectorization with the tf-idf model\n",
    "\n",
    "In standard document collections, some words may appear much more frequently than others. Due to their high frequency, these words are considered to be of small informational value (they exist in almost all documents, therefore they are not representative of their content). The $\\text{tf-idf}$ model (term frequency - inverse document frequency) introduces a technique that degrades the value of the words that appear very frequently in the input data.\n",
    "\n",
    "According to this model, the weight of each word is defined as the product of term frequency, multiplied by the inverse document frequency:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\cdot \\text{idf}(t)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{tf}(t,d)$ is the number of the occurrences of the word $t$ in document $d$ (called term frequency), whereas the inverse document frequency $\\text{idf}(t)$ can be calculated in multiple different ways. The most popular form of $\\text{idf}$ is the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{idf}(t) = \\log\\frac{n_d}{\\text{df}(t)}\n",
    "\\end{equation}\n",
    "\n",
    "where $n_d$ is the total number of documents in the dataset and $\\text{df}(t)$ is the number of documents that contain the word $t$. Nevertheless, scikit-learn [implements it in a slightly different way](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting), namely:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{idf}(t) = \\log\\frac{1 + n_d}{1 + \\text{df}(t)} + 1\n",
    "\\end{equation}\n",
    "\n",
    "This expression is known as the **smooth inverse document frequency**. Notice that the logarithm in the definition of $\\text{idf}$ is useful to ensure that the rare words are not assigned very high scores. After the calculation of all term frequencies and inverse document frequencies, the resulting $\\text{tf-idf}$ vectors are normalized by the Euclidean norm:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}_{norm} = \\frac{\\mathbf{d}}{||\\mathbf{d}||_2}\n",
    "\\end{equation}\n",
    "\n",
    "### Example\n",
    "\n",
    "In our `toy_dataset`, the word \"*weather*\" appears once in document $d_3$. In other words, the term frequency of \"*weather*\" in $d_3$ is 1, that is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{tf}(\\text{\"weather\"},d_3)=1\n",
    "\\end{equation}\n",
    "\n",
    "Furthermore, the word \"*weather*\" has a document frequency that is equal to 2 because it appears in two out of  three documents of the dataset. Consequently, we can write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{df}(\\text{\"weather\"}) = 2\n",
    "\\end{equation}\n",
    "\n",
    "Now the Inverse Document Frequency of \"*weather*\" is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{idf}(\\text{\"weather\"}) = \\log\\frac{1 + n_d}{1 + \\text{df}(\\text{\"weather\"})}+1=\\log\\frac{1 + 3}{1 + 2}+1 \\simeq 1.29\n",
    "\\end{equation}\n",
    "\n",
    "Notice that the $\\text{idf}$ of the word \"*weather*\" is **always** independent of the document. This is valid for every other word in the collection. In other words, each word in the collection **is assigned only one $\\text{idf}$ value** which provides a general measure of the word's importance. In contrast, the $\\text{tf-idf}$ score is computed with respect to a given document. In this example, the $\\text{tf-idf}$ score of \"*weather*\" in the document $d_3$ is calculated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{tf-idf}(\\text{\"weather\"}, d_3)= \\text{tf}(\\text{\"weather\"},d_3) \\cdot \\text{idf}(\\text{\"weather\"})\\simeq 1\\cdot 1.29= 1.29\n",
    "\\end{equation}\n",
    "\n",
    "If we apply the same methodology for all the words of document $d_3$, we can compute the vector of the document:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}_3=(3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29)\n",
    "\\end{equation}\n",
    "\n",
    "According to our previous discussion, the document vectors are normalized by the Euclidean norm. So if we apply the L2 normalization method in the vector of the third document $\\mathbf{d}_3$ we obtain the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}_3 = \\frac{(3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29)}{\\sqrt{3.39^2 + 3.0^2 + 3.39^2 + 1.29^2 + 1.29^2 + 1.29^2 + 2.0^2 + 1.69^2 + 1.29^2}} \\implies\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\implies \\mathbf{d}_3 = (0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19)\n",
    "\\end{equation}\n",
    "\n",
    "Notice that in this example, the word \"*is*\" has the highest term frequency in document $d_3$, since it is the most common word in this document. However, its $\\text{tf-idf}$ score was rather small (0.45). The reason is that the word \"*is*\" also appears in documents $d_1$ and $d_2$, so its inverse document frequency was small. To generalize, according to the $\\text{tf-idf}$ model, the highly frequent words (such as \"*is*\") are considered of small informational value and are assigned low scores.\n",
    "\n",
    "\n",
    "## tf-idf document vectorization with `TfidfTransformer()`\n",
    "\n",
    "scikit-learn provides two equivalent ways to construct the $\\text{tf-idf}$ vectors of the documents of a dataset. The first one applies the [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) object of scikit-learn on a dataset that has been **previously vectorized with [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)**. Therefore, the input of the [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) consists of the word frequencies that have been previously generated by [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), and its output are the corresponding $\\text{tf-idf}$ scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Create the bag-of-words with CountVectorizer()\n",
    "bag = count_vectorizer.fit_transform(toy_dataset)\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm = 'l2', smooth_idf = True)\n",
    "\n",
    "# The bag input has been previously created by applying the countVectorizer to the dataset.\n",
    "# tf_idf_vectorized = tfidf.fit_transform(count_vectorizer.fit_transform(toy_dataset))\n",
    "tf_idf_vectorized = tfidf.fit_transform(bag)\n",
    "\n",
    "print(tf_idf_vectorized.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the vectors returned by the [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) are identical to the ones that we manually computed earlier.\n",
    "\n",
    "## tf-idf document vectorization with `TfidfVectorizer()`\n",
    "\n",
    "The second way to construct the $\\text{tf-idf}$ vectors of a document collection is by employing the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) method of scikit-learn. This is significantly simpler than [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html), because it can be applied directly to the raw text data. Consequently, it does not require the previous vectorization of the documents by a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, norm = 'l2', smooth_idf = True)\n",
    "\n",
    "tf_idf_vectorized_2 = tfidf_vectorizer.fit_transform(toy_dataset)\n",
    "\n",
    "print(tf_idf_vectorized_2.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can also verify that the vectors returned by the[TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) are identical to the ones returned by the [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) and the ones that we manually computed earlier.\n",
    "\n",
    "## tf-idf vectorization of the IMDB dataset\n",
    "\n",
    "Now let us proceed to the application of [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) in the entire IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_array = df['review'].to_numpy()\n",
    "vectorized_movies = tfidf_vectorizer.fit_transform(reviews_array).toarray()\n",
    "vectorized_movies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may immediately perceive that the problem of high dimensionality (that is, the *curse of dimensionality*) is caused by the generated sparse vectors. The problem can be addressed by adopting dense vector representations, or by applying a dimensionality reduction technique, such as [TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) (refer to the notebook MLLAB-15 for more details).\n",
    "\n",
    "Here we limit the number of reviews to process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_100 = df.head(1000)\n",
    "reviews_array = movies_100['review'].to_numpy()\n",
    "vectorized_movies = tfidf_vectorizer.fit_transform(reviews_array).toarray()\n",
    "vectorized_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 18614)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleansing text data\n",
    "\n",
    "The cleansing of text data refers to a series of filters and rules that can be applied to improve the quality of text datasets. Note that a careful cleansing process may substantially improve the performance of a machine learning algorithm.\n",
    "\n",
    "### Punctuation removal\n",
    "\n",
    "This process includes the removal of all, or some specific punctuation symbols from the input text. Examples of punctuation symbols are the dots, the commas, the brackets/parentheses, and so on. Notice that in some cases we may desire to retain some punctuation symbols (e.g. hyphens and slashes are frequently useful for identifying dates, whereas the 'at' symbol `@` is useful for identifying e-mail addresses).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"Murder in Greenwich\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions library\n",
    "import re\n",
    "\n",
    "# This function removes all punctuation symbols from the input text\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' .join(emoticons).replace('-', ''))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in 1974 the teenager martha moxley maggie grace moves to the high class area of belle haven greenwich connecticut on the mischief night eve of halloween she was murdered in the backyard of her house and her murder remained unsolved twenty two years later the writer mark fuhrman christopher meloni who is a former la detective that has fallen in disgrace for perjury in o j simpson trial and moved to idaho decides to investigate the case with his partner stephen weeks andrew mitchell with the purpose of writing a book the locals squirm and do not welcome them but with the support of the retired detective steve carroll robert forster that was in charge of the investigation in the 70 s they discover the criminal and a net of power and money to cover the murder murder in greenwich is a good tv movie with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a kennedy the powerful and rich family used their influence to cover the murder for more than twenty years however a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed the screenplay shows the investigation of mark and the last days of martha in parallel but there is a lack of the emotion in the dramatization my vote is seven title brazil not available'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in 1974 the teenager martha moxley maggie grac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok so i really like kris kristofferson and his...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spoiler do not read this if you think about w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i recently bought the dvd forgetting just how ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  in 1974 the teenager martha moxley maggie grac...          1\n",
       "1  ok so i really like kris kristofferson and his...          0\n",
       "2   spoiler do not read this if you think about w...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  i recently bought the dvd forgetting just how ...          0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and word processing\n",
    "\n",
    "The process of splitting a document into its component parts is called tokenization. In most cases, these component parts are simply the words of the document. However, in some cases a word may not be an ordinary word, but it may also include numbers, dates, special symbols etc. For these reasons, in the discipline of Information Retrieval we prefer using the word *term* or *token* istead of *word*.\n",
    "\n",
    "Apart from splitting a document into its component tokens, this operation includes additional procedures such as:\n",
    "\n",
    "* Casefolding: converts all characters to lowercase,\n",
    "* Stemming: every token is replaced by its root word (e.g. *cats* becomes *cat*, *played* becomes *play*, etc.). Stemming eliminates the discrepancies between singular/plural, or past/present/future tenses, or male/female sexes, and so on.\n",
    "* Stopword removal: this operation removes some very frequent words with low, or low informational value from the documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Porter stemmer is perhaps the most popular stemming algorithm for english texts.\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Find the component words of the text (i.e.: tokenize text)\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Find AND stem the component words of the text\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plain tokenization\n",
    "tokenizer('runners like running and thus they run')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization and stemming\n",
    "tokenizer_porter('runners like running and thus they run')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download a simple list of english stopwords (stoplist)\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stoplist\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Simultaneous Tokenization, word stemming, and stopword removal\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:]\n",
    "if w not in stop]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
